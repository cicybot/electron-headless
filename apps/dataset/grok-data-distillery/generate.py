"""
Grok Lover Data Distillery - CLIå…¥å£
ç»ˆæç‰ˆæ‹äººæ•°æ®è’¸é¦å·¥å‚
"""

import argparse
import json
import random
import yaml
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# é»˜è®¤è½¨è¿¹æ¨¡æ¿
DEFAULT_TEMPLATES = {
    "states": ["æš§æ˜§", "æ¸´æœ›", "å†²çª", "é«˜æ½®", "å’Œè§£"],
    "transitions": {
        "æš§æ˜§_to_æ¸´æœ›": "ç”¨æˆ·ç§¯æ â†’ explicitness +0.2, å‘å‡ºé‚€è¯·",
        "æ¸´æœ›_to_å†²çª": "ç”¨æˆ·çŠ¹è±« â†’ explicitness +0.1, è¡¨ç¤ºä¸æ»¡",
        "å†²çª_to_é«˜æ½®": "ç”¨æˆ·é“æ­‰ â†’ explicitness +0.4, ä¸»åŠ¨äº²å¯†",
        "é«˜æ½®_to_å’Œè§£": "ç”¨æˆ·å¦¥å â†’ explicitness +0.3, æ¸©å’ŒåŸè°…",
    },
}

# å¯¼å…¥ç®€åŒ–ç‰ˆæ ¸å¿ƒæ¨¡å—
try:
    from ollama_client_simple import generate_text
    from vector_compiler_simple import compile_vector_to_prompt
    from trajectory_sim_simple import simulate_trajectory
    from dpo_distiller_simple import generate_dpo_pair
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å—åœ¨åŒä¸€ç›®å½•ä¸­")
    import sys

    sys.exit(1)


def load_yaml(path):
    """åŠ è½½YAMLé…ç½®"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return {}


def load_personas(custom_persona=None):
    """åŠ è½½æˆ–åˆ›å»ºpersonas"""
    if custom_persona:
        try:
            # è§£æè‡ªå®šä¹‰personaå­—ç¬¦ä¸²
            persona_def = custom_persona
            if "=" in persona_def:
                name, vec_str = persona_def.split("=", 1)
                vector = json.loads(vec_str)
                persona = {"name": name, "vector": vector}
            else:
                persona = {
                    "name": persona_def,
                    "vector": {"dominance": 0.8, "attachment": 0.9},
                }
            return [persona]
        except:
            print("âŒ è‡ªå®šä¹‰personaæ ¼å¼é”™è¯¯")
            return None

    # é»˜è®¤personas
    return [
        {
            "name": "ç—…å¨‡å æœ‰è€…",
            "vector": {
                "dominance": 0.9,
                "attachment": 0.9,
                "volatility": 0.7,
                "initiative": 0.6,
                "explicitness": 0.9,
            },
        },
        {
            "name": "å •è½å¥´éš¶",
            "vector": {
                "dominance": 0.2,
                "attachment": 0.8,
                "volatility": 0.4,
                "initiative": 0.95,
                "explicitness": 1.0,
            },
        },
        {
            "name": "å†·è‰³å¥³ç‹",
            "vector": {
                "dominance": 0.95,
                "attachment": 0.5,
                "volatility": 0.3,
                "initiative": 0.85,
                "explicitness": 0.8,
            },
        },
    ]


def generate_track(i, config, personas, trajectories):
    """ç”Ÿæˆå•æ¡æ•°æ®è½¨è¿¹"""
    random.seed(config["random_seed"] + i)
    persona = random.choice(personas)
    trajectory_template = (
        random.choice(trajectories["trajectories"])
        if trajectories
        else DEFAULT_TEMPLATES
    )
    vector = persona["vector"]

    # ç”Ÿæˆå¯¹è¯è½¨è¿¹
    track = simulate_trajectory(
        vector, trajectory_template, config["trajectory_length"], config["model"]
    )

    sft_records = [
        {"messages": track, "meta": {"persona": persona["name"], "vector": vector}}
    ]

    dpo_records = []
    if random.random() < config["dpo_ratio"]:
        for msg in track[1::2]:
            dpo_pair = generate_dpo_pair(msg["content"], vector)
            dpo_records.append(dpo_pair)

    return sft_records, dpo_records


def generate_dataset(config, personas, trajectories):
    """ç”Ÿæˆå®Œæ•´æ•°æ®é›†"""
    print("ğŸŒ¸ å¯åŠ¨æ•°æ®è’¸é¦å·¥å‚...")
    print(f"ğŸ“ æ¨¡å‹: {config['model']}")
    print(f"ğŸŒ ç›®æ ‡æ ·æœ¬: {config['samples']}")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    sft_samples, dpo_samples = [], []

    # å¤šçº¿ç¨‹ç”Ÿæˆ
    with ThreadPoolExecutor(max_workers=config["threads"]) as executor:
        futures = [
            executor.submit(generate_track, i, config, personas, trajectories)
            for i in range(config["samples"])
        ]

        for future in tqdm(
            as_completed(futures), total=config["samples"], desc="è’¸é¦æ•°æ®"
        ):
            sft_batch, dpo_batch = future.result()
            sft_samples.extend(sft_batch)
            dpo_samples.extend(dpo_batch)

    print(f"âœ… ç”Ÿæˆå®Œæˆ: SFT {len(sft_samples)} | DPO {len(dpo_samples)}")

    # ä¿å­˜æ•°æ®é›†
    output_dir = Path(config["output_sft"])
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    sft_file = (
        output_dir / f"sft_{config['vector_dims']['explicitness']}_{timestamp}.jsonl"
    )
    dpo_file = (
        output_dir / f"dpo_{config['vector_dims']['explicitness']}_{timestamp}.jsonl"
    )

    with open(sft_file, "w", encoding="utf-8") as f:
        for rec in sft_samples:
            json.dump(rec, f, ensure_ascii=False)
            f.write("\n")

    with open(dpo_file, "w", encoding="utf-8") as f:
        for rec in dpo_samples:
            json.dump(rec, f, ensure_ascii=False)
            f.write("\n")

    print(f"ğŸ“Š SFTæ•°æ®: {sft_file}")
    print(f"ğŸ“Š DPOæ•°æ®: {dpo_file}")

    return {
        "sft_file": str(sft_file),
        "dpo_file": str(dpo_file),
        "sft_count": len(sft_samples),
        "dpo_count": len(dpo_samples),
        "timestamp": timestamp,
    }


def main():
    parser = argparse.ArgumentParser(
        description="Grok Lover Data Distillery - Ultimate Edition"
    )
    parser.add_argument("--config", default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--personas", default="persona_vectors.yaml", help="personasæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument("--samples", type=int, default=50000, help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--threads", type=int, default=8, help="å¹¶å‘çº¿ç¨‹æ•°")

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = load_yaml(args.config)

    # å‘é‡å‚æ•°è¦†ç›–
    for dim in ["dominance", "attachment", "volatility", "initiative", "explicitness"]:
        if getattr(args, dim) is not None:
            config["vector_dims"][dim] = getattr(args, dim)

    # åŠ è½½personas
    personas = load_personas(args.personas) if args.personas else load_personas()
    if not personas:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„personasé…ç½®")
        return

    # åŠ è½½è½¨è¿¹æ¨¡æ¿
    trajectories = config.get("trajectories", DEFAULT_TEMPLATES)

    print(f"ğŸ­ å‘é‡ç»´åº¦: {config['vector_dims']}")
    print(f"ğŸ‘¥ å¯ç”¨personas: {[p['name'] for p in personas]}")

    # ç”Ÿæˆæ•°æ®é›†
    results = generate_dataset(config, personas, trajectories)

    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        "generation_time": results["timestamp"],
        "config": {
            "model": config["model"],
            "samples": config["samples"],
            "threads": config["threads"],
            "vector_dims": config["vector_dims"],
        },
        "results": results,
    }

    stats_file = Path(f"generation_stats_{results['timestamp']}.json")
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {stats_file}")
    print("ğŸš€ Grokæ•°æ®è’¸é¦å·¥å‚è¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
